{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "def load_housing_data(housing_path=''): \n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\") \n",
    "    return pd.read_csv(csv_path)\n",
    "housing = load_housing_data()\n",
    "\n",
    "# Print dataset function\n",
    "def print_sample_and_shape(data=housing, sample_size=10):\n",
    "    print(\"Data: \",data[:sample_size])\n",
    "    print(\"Shape: \",data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBUG_P_2 = False\n",
    "\n",
    "# Adding column based on median_income with less cases\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5) # create category based on median income as an integer using ceil()\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True) # put every sample of income_cat >= 5 into category 5\n",
    "\n",
    "# Splitting into train and test set\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) # shuffle and split with representative distribution\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    train_set = housing.loc[train_index]\n",
    "    test_set = housing.loc[test_index]\n",
    "\n",
    "# Printing dataset\n",
    "if DEBUG_P_2:\n",
    "    print_sample_and_shape(train_set)\n",
    "    print_sample_and_shape(test_set)\n",
    "\n",
    "# Visualizing the input\n",
    "if DEBUG_P_2:\n",
    "    housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
    "    plt.show()\n",
    "\n",
    "# Checking correlation\n",
    "if DEBUG_P_2:\n",
    "    corr_matrix = housing.corr()\n",
    "    print(corr_matrix[\"median_house_value\"].sort_values(ascending=False))\n",
    "\n",
    "# Print income_cat proportion\n",
    "# print(housing[\"income_cat\"].value_counts() / len(housing))\n",
    "\n",
    "# Revert data\n",
    "for set in (train_set, test_set): \n",
    "    set.drop([\"income_cat\"], axis=1, inplace=True)\n",
    "housing.drop([\"income_cat\"], axis=1, inplace=True)\n",
    "\n",
    "housing = train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBUG_P_3 = False\n",
    "\n",
    "# (a) Sanitizing\n",
    "\n",
    "# Replace nulls\n",
    "from sklearn.impute import SimpleImputer \n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "# Extract only number features\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1) # ocean_proximity is a string, only numbers are to be kept\n",
    "imputer.fit(housing_num)\n",
    "housing_num_filled = pd.DataFrame(imputer.transform(housing_num), columns=housing_num.columns)\n",
    "\n",
    "# Encode String as OneHot binary list\n",
    "# Useful encoders: LabelEncoder (text to num), OneHotEncoder (num to 1Hot), LabelBinarizer (text to 1Hot)\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "housing_cat = encoder.fit_transform(housing[\"ocean_proximity\"])\n",
    "# print(housing_cat[:10])\n",
    "\n",
    "# (b) Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "housing_num_scaled = scaler.fit_transform(housing_num_filled)\n",
    "# print(housing_num_scaled[:10])\n",
    "\n",
    "# (c) Custom transformers\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room \n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do \n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix] \n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix] \n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "class FixLabelBinarizer(TransformerMixin):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.encoder = LabelBinarizer(*args, **kwargs)\n",
    "    def fit(self, x, y=0):\n",
    "        self.encoder.fit(x)\n",
    "        return self\n",
    "    def transform(self, x, y=0):\n",
    "        return self.encoder.transform(x)\n",
    "\n",
    "# (c) Pipelining\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "# Numerical pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(num_attribs)), # Select features\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")), # Fill nulls\n",
    "    ('attribs_adder', CombinedAttributesAdder()), # Add combined features\n",
    "    ('std_scaler', StandardScaler()), # Feature scaling\n",
    "])\n",
    "\n",
    "# String pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(cat_attribs)), # Select string features\n",
    "    ('label_binarizer', FixLabelBinarizer()), # Encode as 1Hot\n",
    "])\n",
    "\n",
    "# Union\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline),\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "if DEBUG_P_3:\n",
    "    print(housing_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBUG_P_4 = False\n",
    "\n",
    "# Print predictions\n",
    "def print_predictions_samples(pred, num):\n",
    "    some_data = housing.iloc[:num]\n",
    "    some_labels = housing_labels.iloc[:num]\n",
    "    some_data_prepared = full_pipeline.transform(some_data)\n",
    "    print(\"Predictions:\\t\", pred.predict(some_data_prepared)) \n",
    "    print(\"Labels:\\t\\t\", list(some_labels))\n",
    "\n",
    "# RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def fit_and_print_rmse(pred):\n",
    "    pred.fit(housing_prepared, housing_labels)\n",
    "    housing_predictions = pred.predict(housing_prepared)\n",
    "    error = np.sqrt(mean_squared_error(housing_labels, housing_predictions))\n",
    "    print(\"Error of \",pred,\" is: \",error)\n",
    "\n",
    "# Linear regression\n",
    "from sklearn.linear_model import LinearRegression \n",
    "lin_reg = LinearRegression()\n",
    "if DEBUG_P_4:\n",
    "    fit_and_print_rmse(lin_reg)\n",
    "\n",
    "# DecisionTree regression\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "tree_reg = DecisionTreeRegressor()\n",
    "if DEBUG_P_4:\n",
    "    fit_and_print_rmse(tree_reg)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "if DEBUG_P_4:\n",
    "    fit_and_print_rmse(forest_reg)\n",
    "\n",
    "# Cross validation score:\n",
    "# It splits train set into 'cv' folds. \n",
    "# Then, for each of them, it performs a fitting taking it out \n",
    "# and generates a score taking the current set as the validation set.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def fit_and_print_cv_error(pred):\n",
    "    scores = cross_val_score(pred, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    rmse_scores = np.sqrt(-scores)\n",
    "    print(\"Scores:\", rmse_scores)\n",
    "    print(\"Mean:\", rmse_scores.mean())\n",
    "    print(\"Standard deviation:\", rmse_scores.std())\n",
    "\n",
    "if DEBUG_P_4:\n",
    "    fit_and_print_cv_error(forest_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 5: Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBUG_P_5 = False\n",
    "\n",
    "# Hyperparameter tuning\n",
    "# Useful classes: RandomizedSearchCV, GridSearchCV.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "if DEBUG_P_5:\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_estimator_)\n",
    "    cvres = grid_search.cv_results_\n",
    "    for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]): \n",
    "        print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 6: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "X_test = test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = test_set[\"median_house_value\"].copy()\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse) # => evaluates to 48,209.6\n",
    "print(\"Final RMSE: \", final_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
