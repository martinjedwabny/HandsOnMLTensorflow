{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 11: Training Deep Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients Problems\n",
    "\n",
    "* Gradients often get smaller and smaller as the algorithm progresses down to the lower layers. \n",
    "* As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged.\n",
    "* In some cases, the opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is the exploding gradients problem. It is mostly encountered in recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier and He Initialization\n",
    "\n",
    "* The variance of the outputs of each layer should be equal to the variance of its inputs.\n",
    "* The gradients should have equal variance before and after flowing through a layer in the reverse direction.\n",
    "\n",
    "Glorot and Bengio (2010) proposed the following random initialization distributions for the edges weights.\n",
    "\n",
    "$N(0,\\sigma)$ with $\\sigma = \\sqrt{\\frac{2}{n_{inputs} + n_{outputs}}}$ or $U(-r,r)$ with $r = \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}$\n",
    "\n",
    "* This strategy works well with Logistic activations. For the hyperbolic tangent, a constant 4 is multiplied in $\\sigma$ and $r$, while ReLU activations work better with a $\\sqrt{2}$ constant.\n",
    "* The He initialization is quite similar but removes the $n_{outputs}$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU variants\n",
    "\n",
    "* During training, some neurons die, they stop outputting anything other than 0.\n",
    "* When this happen, the neuron is unlikely to come back to life since the gradient of the ReLU function is 0 when its input is negative.\n",
    "\n",
    "To solve this problem, one can use a variant of the ReLU function, such as the leaky ReLU with $\\alpha \\in (0,1)$ (tipically 0.01 or 0.2):\n",
    "\n",
    "$LeakyReLU_\\alpha(z) = max(\\alpha z, z)$\n",
    "\n",
    "Another popular variant is the Exponential LU, which is slower to compute but is smooth in all the domain and often compensates with convergence speed:\n",
    "\n",
    "$ELU(z) = \\begin{cases} \\alpha (e^{z} - 1) & \\text {if } z < 0 \\\\ z & \\text{if } z \\geq 0 \\end{cases}$\n",
    "\n",
    "#### Note\n",
    "* In TF, to specify ELU activation: hidden1 = fully_connected(X, n_hidden1, activation_fn=tf.nn.elu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJG3/D3Rdy9C0vz/CAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1060167f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "alpha = 1\n",
    "z = np.arange(-2.0, 2.0, 0.01)\n",
    "f = (z < 0) * alpha * (np.exp(z) - 1) + (z >= 0) * z\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(z, f, 'b-')\n",
    "ax.set(xlabel='z', ylabel='f(z)', title='ELU')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "* Vanishing/exploding gradients problems can come back during training.\n",
    "* Sergey Ioffe and Christian Szegedy (2015) proposed a technique called Batch Normalization (BN).\n",
    "* The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling and shifting the result using two new parameters per layer.\n",
    "\n",
    "Concretely, before applying the activation function (if any) the following operation is performed in each layer:\n",
    "\n",
    "$\\hat x^{(i)} = S * (\\frac{x^{(i)} - x_{batch}}{\\sigma_{batch}}) + B$ (where $x_{batch},\\sigma_{batch}$ are the empirical mean/stdev of the current batch and $S,B$ are the scale and bias to be learned).\n",
    "\n",
    "#### Notes\n",
    "\n",
    "* It improves convergence so much that Sigmoid and Tanh functions are possible to use when paired with BN. \n",
    "* Apart from speed, it reduces overfitting and improves accuracy.\n",
    "* However, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "bn_params = {\n",
    "    'is_training': is_training, # use mini-batch mean/std (training) or running averages (testing)\n",
    "    'decay': 0.99, # exponential decay to compute the running averages\n",
    "    'updates_collections': None, # update the running averages right before batch normalization in training\n",
    "    'scale': True, # if activation is other than None or ReLU,\n",
    "    'center': True, \n",
    "    'reuse': False,\n",
    "}\n",
    "\n",
    "with tf.contrib.framework.arg_scope([fully_connected], normalizer_fn=batch_norm, normalizer_params=bn_params):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")\n",
    "    logits = fully_connected(hidden2, n_outputs, scope=\"outputs\", activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping\n",
    "* A popular technique to lessen the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "learning_rate = 0.2\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        training_op = optimizer.minimize(loss)\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Accuracy 0.9572167\n",
      "Epoch 1 Accuracy 0.9741667\n",
      "Epoch 2 Accuracy 0.98291665\n",
      "Epoch 3 Accuracy 0.98908335\n",
      "Epoch 4 Accuracy 0.9920833\n",
      "Epoch 5 Accuracy 0.99516666\n",
      "Epoch 6 Accuracy 0.99663335\n",
      "Epoch 7 Accuracy 0.99801666\n",
      "Epoch 8 Accuracy 0.99883336\n",
      "Epoch 9 Accuracy 0.99945\n"
     ]
    }
   ],
   "source": [
    "from aux import create_mini_batches\n",
    "\n",
    "X_train = X_train.reshape(-1, n_inputs)\n",
    "y_train = y_train.reshape(-1)\n",
    "\n",
    "X_test = X_test.reshape(-1, n_inputs)\n",
    "y_test = y_test.reshape(-1)\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = X_train.shape[0] // batch_size\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in create_mini_batches(X_train, y_train, n_batches, n_inputs, 1): \n",
    "            sess.run(training_op, feed_dict={is_training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_score = accuracy.eval(feed_dict={is_training: True, X: X_train, y: y_train})\n",
    "        print(\"Epoch\", epoch, \"Accuracy\", accuracy_score)\n",
    "        tf.train.Saver(var_list=tf.global_variables()).save(sess, \"./c11_bn_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./c11_bn_final.ckpt\n",
      "Target     [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1]\n",
      "Prediction [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1]\n",
      "Accuracy 0.9791\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.train.Saver().restore(sess, \"./c11_bn_final.ckpt\")\n",
    "    y_pred = sess.run([tf.math.argmax(logits, axis=1)], feed_dict={X: X_test, is_training: True})\n",
    "    print(\"Target    \", y_test[:30])\n",
    "    print(\"Prediction\", y_pred[0][:30])\n",
    "    print (\"Accuracy\", np.sum(y_pred == y_test) / len(y_pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_trasnfer_nn(original_w, original_b):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") \n",
    "    hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "    hidden2 = fully_connected(X, n_hidden2, scope=\"hidden2\")\n",
    "    logits = fully_connected(X, n_outputs, scope=\"logits\")\n",
    "\n",
    "    # Get a handle on the variables created by fully_connected()\n",
    "    with tf.variable_scope(\"\", default_name=\"\", reuse=True): # root scope\n",
    "        hidden1_weights = tf.get_variable(\"hidden1/weights\")\n",
    "        hidden1_biases = tf.get_variable(\"hidden1/biases\")\n",
    "        \n",
    "    # Create nodes to assign arbitrary values to the weights and biases\n",
    "    original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "    original_biases = tf.placeholder(tf.float32, shape=(n_hidden1))\n",
    "    assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "    assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess: \n",
    "        sess.run(init)\n",
    "        sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w}) \n",
    "        sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "#         train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum optimizer\n",
    "\n",
    "* Instead of following the gradient through a constant learning rate, use the gradient as acceleration.\n",
    "* Algorithm: ($\\alpha > 0$ learning rate, $\\beta \\in [0,1]$ momentum friction, $m_{init}=0$)\n",
    "    \n",
    "    $m \\leftarrow \\beta m + \\alpha \\nabla_\\theta cost(\\theta)$\n",
    "    \n",
    "    $\\theta^{new} \\leftarrow \\theta - m$\n",
    "    \n",
    "* In TF: \n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient\n",
    "\n",
    "* Almost always faster than the Momentum optimizer as-is.\n",
    "* Algorithm:\n",
    "    \n",
    "    $m \\leftarrow \\beta m + \\alpha \\nabla_\\theta cost(\\theta + \\beta m)$\n",
    "    \n",
    "    $\\theta^{new} \\leftarrow \\theta - m$\n",
    "    \n",
    "* In TF:\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "* Uses intuitions of Momentum and RMSProp.\n",
    "* It is almost always the best option so far in the literature.\n",
    "* Algorithm:\n",
    "    \n",
    "    $m \\leftarrow \\beta_1 m + (1-\\beta_1) \\nabla_\\theta cost(\\theta + \\beta m)$\n",
    "    \n",
    "    $s \\leftarrow \\beta_2 s + (1-\\beta_2) \\nabla_\\theta cost(\\theta + \\beta m)^2$ (element wise)\n",
    "    \n",
    "    $m \\leftarrow \\frac{m}{1-\\beta_1^T}$ (kickstarting fast)\n",
    "    \n",
    "    $s \\leftarrow \\frac{s}{1-\\beta_2^T}$ (kickstarting fast)\n",
    "    \n",
    "    $\\theta^{new} \\leftarrow \\theta - \\alpha m / \\sqrt{s+\\epsilon}$ (element wise)\n",
    "    \n",
    "    \n",
    "* Hyperparameters: $\\alpha,\\beta_1,\\beta_2, \\epsilon$ control learning rate, momentum decay, scaling decay and smoothing term. \n",
    "\n",
    "\n",
    "* Tipically TFs standard values work well ($\\beta_1=0.9,\\beta_2=0.999,\\epsilon=10^{-8}$).\n",
    "\n",
    "\n",
    "* In TF:\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "* As the name suggests, just save multiple partially trained instances of the parameters.\n",
    "* Then select the one that generalizes the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $l_1$ and $l_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reg_layers(X, y, n_hidden1, n_hidden2, n_outputs, scale=0.01):\n",
    "    with arg_scope([fully_connected],weights_regularizer=tf.contrib.layers.l1_regularizer(scale=scale)):\n",
    "        # Create layers\n",
    "        hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "        hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")\n",
    "        logits = fully_connected(hidden2, n_outputs, activation_fn=None,scope=\"out\")\n",
    "        # Define loss\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "* At every training step, every neuron (including input neurons but excluding output neurons) has a probability p of being temporarily “dropped out”.\n",
    "* It will be entirely ignored during this training step, but it may be active during the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import dropout \n",
    "\n",
    "def create_drop_layers(X, y, n_hidden1, n_hidden2, n_outputs, p=0.5):\n",
    "    with arg_scope([fully_connected],weights_regularizer=tf.contrib.layers.l1_regularizer(scale=scale)):\n",
    "        is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "        # Create layers\n",
    "        X_drop = dropout(X, p, is_training=is_training)\n",
    "        hidden1 = fully_connected(X_drop, n_hidden1, scope=\"hidden1\")\n",
    "        hidden1_drop = dropout(hidden1, keep_prob, is_training=is_training)\n",
    "        hidden2 = fully_connected(hidden1_drop, n_hidden2, scope=\"hidden2\")\n",
    "        hidden2_drop = dropout(hidden2, keep_prob, is_training=is_training)\n",
    "        logits = fully_connected(hidden2_drop, n_outputs, activation_fn=None,scope=\"outputs\")\n",
    "        # Define loss\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "* Generating new training instances from existing ones, artificially boosting the size of the training set.\n",
    "* For example, if your model is meant to classify pictures of mushrooms, you can slightly shift, rotate, and resize every picture in the training set by various amounts and add the resulting pictures to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default DNN configuration\n",
    "\n",
    "* Initialization: He\n",
    "* Activation function: ELU\n",
    "* Normalization: Batch normalization\n",
    "* Regularization: Dropout\n",
    "* Optimizer: Adam\n",
    "* Learning rate schedule: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* If you can’t find a good learning rate (convergence was too slow, so you increased the training rate, and now convergence is fast but the network’s accuracy is sub‐ optimal), then you can try adding a learning schedule such as exponential decay.\n",
    "* If your training set is a bit too small, you can implement data augmentation.\n",
    "* If you need a sparse model, you can add some l1 regularization to the mix (and optionally zero out the tiny weights after training). If you need an even sparser model, you can try using FTRL instead of Adam optimization, along with l1 reg‐ ularization.\n",
    "* If you need a lightning-fast model at runtime, you may want to drop Batch Nor‐ malization, and possibly replace the ELU activation function with the leaky ReLU. Having a sparse model will also help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
