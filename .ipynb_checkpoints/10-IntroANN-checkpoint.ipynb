{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Introduction to Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ANN (Perceptron)\n",
    "\n",
    "A perceptron is one of the simplest ANN architectures. \n",
    "* Each input is represented by a neuron.\n",
    "* Each output also has a neuron.\n",
    "* Every input neuron is connected to every output neuron.\n",
    "\n",
    "Then, an edge between input neuron $i$ and output neuron $j$ has weight $w_{i,j}$.\n",
    "\n",
    "Thus, the output/hypothesis $j$ can be calculated as: \n",
    "\n",
    "$h_w^j(x) = step(\\sum\\limits_{i=1,...,n}w_{i,j}x_i) = step(w_{*,j}^T x)$\n",
    "\n",
    "Where step maps a real number to the 0, 1 and/or -1. For instance, step could be either:\n",
    "\n",
    "$heavyside(z) = \\begin{cases} 0 & \\text{ if } z < 0 \\\\ 1 & otherwise \\end{cases}$\n",
    "$sgn(z) = \\begin{cases} -1 & \\text{ if } z < 0 \\\\ \\ 0 & \\text{ if } z = 0 \\\\ \\ 1 & otherwise \\end{cases}$\n",
    "\n",
    "The learning rule is:\n",
    "\n",
    "$w_{i,j}' = w_{i,j} + r (\\hat y_j - y_j)x_i$ ($r$ the learning rate).\n",
    "\n",
    "#### Note:\n",
    "* Cannot learn complex patterns.\n",
    "* With these activation function, only suitable for classification.\n",
    "* If the data is linearly separable, then it converges according to Rosenblastt's theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Prediction [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris \n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width \n",
    "y = (iris.target == 0).astype(np.int) # Iris Setosa?\n",
    "\n",
    "# Train classifier\n",
    "per_clf = Perceptron(random_state=42, max_iter=1000, tol=1e-3)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "# Example\n",
    "print(\"Target\", y[10:100])\n",
    "print(\"Prediction\", per_clf.predict(X[10:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic DNN (Multi-layer perceptron)\n",
    "\n",
    "* Concept: stacking multiple perceptrons.\n",
    "* Also called a DNN (at least one hidden layer).\n",
    "* Training: for each training instance the backpropagation algorithm first makes a prediction (forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally slightly tweaks the connection weights to reduce the error (Gradient Descent step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "\n",
    "Instead of the step function, and in order to implement Gradient Descent to update the weights of the network, the following activation functions were proposed:\n",
    "\n",
    "* Logistic: $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "* Hyperbolic tangent: $tanh(z)=2\\sigma(2z)-1$\n",
    "* Rectified Linear Unit: $ReLU(z)=max(0,z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/_5/tlg_1lsd5r76sx6dp5b399n00000gn/T/tmput1hh_3f\n",
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_save_checkpoints_steps': None, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_model_dir': '/var/folders/_5/tlg_1lsd5r76sx6dp5b399n00000gn/T/tmput1hh_3f', '_task_id': 0, '_keep_checkpoint_max': 5, '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12e4df208>, '_master': '', '_tf_random_seed': None, '_is_chief': True, '_train_distribute': None, '_num_worker_replicas': 0, '_device_fn': None, '_session_config': None, '_task_type': None, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_protocol': None, '_environment': 'local'}\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/_5/tlg_1lsd5r76sx6dp5b399n00000gn/T/tmput1hh_3f/model.ckpt.\n",
      "INFO:tensorflow:loss = 4.569376, step = 1\n",
      "INFO:tensorflow:global_step/sec: 338.584\n",
      "INFO:tensorflow:loss = 0.038750075, step = 101 (0.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.707\n",
      "INFO:tensorflow:loss = 0.009768624, step = 201 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 493.564\n",
      "INFO:tensorflow:loss = 0.0067967884, step = 301 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 495.523\n",
      "INFO:tensorflow:loss = 0.008128978, step = 401 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 483.641\n",
      "INFO:tensorflow:loss = 0.003567285, step = 501 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 500.606\n",
      "INFO:tensorflow:loss = 0.0022189585, step = 601 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 484.466\n",
      "INFO:tensorflow:loss = 0.0014568813, step = 701 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.824\n",
      "INFO:tensorflow:loss = 0.0019877194, step = 801 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 483.62\n",
      "INFO:tensorflow:loss = 0.0013543209, step = 901 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 369.066\n",
      "INFO:tensorflow:loss = 0.0009958615, step = 1001 (0.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.548\n",
      "INFO:tensorflow:loss = 0.00043138553, step = 1101 (0.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.235\n",
      "INFO:tensorflow:loss = 0.001184838, step = 1201 (0.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 462.871\n",
      "INFO:tensorflow:loss = 0.000837525, step = 1301 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 478.766\n",
      "INFO:tensorflow:loss = 0.00026932068, step = 1401 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 401.469\n",
      "INFO:tensorflow:loss = 0.0006555624, step = 1501 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 383.564\n",
      "INFO:tensorflow:loss = 0.0010626625, step = 1601 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 392.001\n",
      "INFO:tensorflow:loss = 0.00040195952, step = 1701 (0.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.638\n",
      "INFO:tensorflow:loss = 0.00039142944, step = 1801 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 472.034\n",
      "INFO:tensorflow:loss = 0.00042677188, step = 1901 (0.212 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /var/folders/_5/tlg_1lsd5r76sx6dp5b399n00000gn/T/tmput1hh_3f/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0003684368.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(params={'dropout': None, 'embedding_lr_multipliers': None, 'hidden_units': [300, 100], 'gradient_clip_norm': None, 'activation_fn': <function relu at 0x1267b3ea0>, 'feature_columns': (_RealValuedColumn(column_name='', dimension=64, default_value=None, dtype=tf.float64, normalizer=None),), 'input_layer_min_slice_size': None, 'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x12e4df390>, 'optimizer': None})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.15, \n",
    "    random_state=42)\n",
    "\n",
    "# Train classifier\n",
    "import tensorflow as tf\n",
    "feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(\n",
    "    hidden_units=[300, 100], \n",
    "    n_classes=10,\n",
    "    feature_columns=feature_columns)\n",
    "dnn_clf.fit(\n",
    "    x=X_train, \n",
    "    y=y_train, \n",
    "    batch_size=50, \n",
    "    steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target [1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9 4 7 6 6 9 1 3 6 1 3\n",
      " 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4 7 0 4 5 5 9 9 0 2 3\n",
      " 8 0 6 4 4 9 1 2 8 3 5 2 9 0 4 4]\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/_5/tlg_1lsd5r76sx6dp5b399n00000gn/T/tmput1hh_3f/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Prediction [1, 9, 4, 0, 4, 2, 3, 7, 8, 8, 4, 3, 9, 7, 5, 6, 3, 5, 6, 3, 4, 9, 1, 4, 4, 6, 9, 4, 7, 6, 6, 9, 1, 3, 6, 1, 3, 0, 6, 5, 5, 1, 9, 5, 6, 0, 9, 0, 0, 1, 0, 4, 5, 2, 4, 5, 7, 0, 7, 5, 9, 5, 5, 4, 7, 0, 4, 5, 5, 9, 9, 0, 2, 3, 8, 0, 6, 4, 4, 9, 1, 2, 8, 3, 5, 2, 9, 0, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print(\"Target\", y_test[10:100])\n",
    "print(\"Prediction\", list(dnn_clf.predict(X_test[10:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-08829dcdaaed>:2: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-18-08829dcdaaed>:2: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-21T14:29:49Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/_5/tlg_1lsd5r76sx6dp5b399n00000gn/T/tmput1hh_3f/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-21-14:29:49\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9777778, global_step = 2000, loss = 0.08264495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9777778, 'global_step': 2000, 'loss': 0.08264495}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "dnn_clf.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom DNN (using TF lower-level API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "n_inputs = 64\n",
    "n_hidden1 = 300 \n",
    "n_hidden2 = 100 \n",
    "n_outputs = 10\n",
    "\n",
    "# Create a nn layer: Input: (X: placeholder, n_neurons: int, name: string, activation: string)\n",
    "def neuron_layer(X, n_neurons, name, activation=None): \n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev) \n",
    "        W = tf.Variable(init, name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z) \n",
    "        else:\n",
    "            return z\n",
    "\n",
    "# Create nn using the nn layer creation function 'neuron_layer' defined above\n",
    "def create_nn_1(X):\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\") \n",
    "        hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\") \n",
    "        logits = neuron_layer(hidden2, n_outputs, \"outputs\")\n",
    "        return logits\n",
    "\n",
    "# Create nn using TF's 'fully_connected' function\n",
    "def create_nn_2(X):\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\") \n",
    "        hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\") \n",
    "        logits = fully_connected(hidden2, n_outputs, scope=\"outputs\", activation_fn=None)\n",
    "        return logits\n",
    "    \n",
    "# Calculate loss as mean of cross entropy\n",
    "def calc_loss(y,logits):\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        return loss\n",
    "\n",
    "# Create optimizer\n",
    "def create_optimizer(X,y,learning_rate,loss,logits):\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate) \n",
    "        training_op = optimizer.minimize(loss)\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    return training_op, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.7924034 Test accuracy: 0.8185185\n",
      "50 Train accuracy: 1.0 Test accuracy: 0.9777778\n",
      "100 Train accuracy: 1.0 Test accuracy: 0.9814815\n",
      "150 Train accuracy: 1.0 Test accuracy: 0.9814815\n"
     ]
    }
   ],
   "source": [
    "# Run DNN\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "tf.reset_default_graph()\n",
    "n_inputs = 64\n",
    "n_hidden1 = 300 \n",
    "n_hidden2 = 100 \n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "logits = create_nn_2(X)\n",
    "loss = calc_loss(y,logits)\n",
    "training_op, accuracy = create_optimizer(X,y,0.1,loss,logits)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 200\n",
    "batch_size = 50\n",
    "n_batches = (X_train.shape[0] // batch_size)\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        batches = ShuffleSplit(n_splits=n_batches, test_size=0.0, random_state=42)\n",
    "        for train_index, _ in batches.split(X_train):\n",
    "            X_batch, y_batch = X_train[train_index], y_train[train_index]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch%50 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_train, y: y_train})\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test}) \n",
    "            print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Restore model\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    Z = logits.eval(feed_dict={X: X_test})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target [6 9 3 7 2 1 5 2 5 2]\n",
      "Prediction [6 9 3 7 2 1 5 2 5 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Target\", y_test[:10])\n",
    "print(\"Prediction\", y_pred[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
